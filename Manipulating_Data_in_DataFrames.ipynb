{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating Data in DataFrames\n",
    "\n",
    "In this lecture we will learn how to manipulate data in dataframes. You will need these techniques to accomplish some of the following tasks:\n",
    "\n",
    " - Change data types when they are incorrectly interpretted\n",
    " - Clean your data\n",
    " - Create new columns\n",
    " - Rename columns\n",
    " - Extract or Create New Values\n",
    " \n",
    "We will also cover how to manipulate arrays in this lecture as well. \n",
    "\n",
    "#### So let's get started!\n",
    "\n",
    "First we will create our spark instance as we need to do at the start of every project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working with 1 core(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://orcuns-mbp-2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Manipulate</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f87711554d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"Manipulate\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark's Immutability\n",
    "\n",
    "Before we get started, let's first take a moment to discuss the concept of Sparks Immutability. Spark DataFrames are immutable. What does that mean? Let's take a look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|   Abraham|  Lincoln|\n",
      "+----------+---------+\n",
      "\n",
      "None\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "names = spark.createDataFrame([('Abraham','Lincoln')], ['first_name', 'last_name'])\n",
    "print(names.show())\n",
    "print(names.rdd.id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the dataframe id\n",
    "\n",
    "Now add a column to the dataframe and keep calling it the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a col\n",
    "from pyspark.sql.functions import *\n",
    "names = names.select(names.first_name,names.last_name,concat_ws(' ', names.first_name, names.last_name).alias('full_name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see how the id of the dataframe changes but the name of the dataframe is still the same. So you can go back and reload the old id if you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------+\n",
      "|first_name|last_name|      full_name|\n",
      "+----------+---------+---------------+\n",
      "|   Abraham|  Lincoln|Abraham Lincoln|\n",
      "+----------+---------+---------------+\n",
      "\n",
      "None\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "print(names.show())\n",
    "print(names.rdd.id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading and old rdd by id\n",
    "\n",
    "First set up a checkpoint folder\n",
    "\n",
    " - !pwd\n",
    " - !mkdir <pwd_output>/checkpoints\n",
    " - spark.sparkContext.setCheckpointDir('<pwd_output>/checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/orcuncanlilar/Eden/PySpark Essentials for Data Scientists/Jupyter Notebooks/PySpark DataFrame Essentials\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir '/Users/orcuncanlilar/Eden/PySpark Essentials for Data Scientists/Jupyter Notebooks/PySpark DataFrame Essentials/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setCheckpointDir('/Users/orcuncanlilar/Eden/PySpark Essentials for Data Scientists/Jupyter Notebooks/PySpark DataFrame Essentials/checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|   Abraham|  Lincoln|\n",
      "+----------+---------+\n",
      "\n",
      "None\n",
      "72\n",
      "+----------+---------+---------------+\n",
      "|first_name|last_name|      full_name|\n",
      "+----------+---------+---------------+\n",
      "|   Abraham|  Lincoln|Abraham Lincoln|\n",
      "+----------+---------+---------------+\n",
      "\n",
      "None\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([('Abraham','Lincoln')], ['first_name', 'last_name'])\n",
    "df1.checkpoint()\n",
    "print(df1.show())\n",
    "print(df1.rdd.id())\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "df2 = df1.select(names.first_name,names.last_name,concat_ws(' ', names.first_name, names.last_name).alias('full_name'))\n",
    "df2.checkpoint()\n",
    "print(names.show())\n",
    "print(names.rdd.id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\"\\nmismatched input '/' expecting <EOF>(line 1, pos 11)\\n\\n== SQL ==\\ncheckpoints/87b411a8-19e3-402a-86a7-cfac0a4a6d14/rdd-40/*\\n-----------^^^\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o724.table.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input '/' expecting <EOF>(line 1, pos 11)\n\n== SQL ==\ncheckpoints/87b411a8-19e3-402a-86a7-cfac0a4a6d14/rdd-40/*\n-----------^^^\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseTableIdentifier(ParseDriver.scala:49)\n\tat org.apache.spark.sql.SparkSession.table(SparkSession.scala:624)\n\tat org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:674)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:844)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-5f3c699f0aa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoints/87b411a8-19e3-402a-86a7-cfac0a4a6d14/rdd-40/*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mtable\u001b[0;34m(self, tableName)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \"\"\"\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \"\\nmismatched input '/' expecting <EOF>(line 1, pos 11)\\n\\n== SQL ==\\ncheckpoints/87b411a8-19e3-402a-86a7-cfac0a4a6d14/rdd-40/*\\n-----------^^^\\n\""
     ]
    }
   ],
   "source": [
    "reload = spark.read.something('checkpoints/87b411a8-19e3-402a-86a7-cfac0a4a6d14/rdd-40/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|��\u0000\u0005sr\u00003org.apach...|\n",
      "|                ��\u0000\u0005|\n",
      "|                ��\u0000\u0005|\n",
      "|                ��\u0000\u0005|\n",
      "|                ��\u0000\u0005|\n",
      "|                ��\u0000\u0005|\n",
      "|                ��\u0000\u0005|\n",
      "|                ��\u0000\u0005|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'rdd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-fb95ba716345>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# test.rdd.getCheckpointFile()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misCheckpointed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'rdd'"
     ]
    }
   ],
   "source": [
    "names.rdd.checkpoint()\n",
    "names.rdd.getCheckpointFile()\n",
    "names.rdd.isCheckpointed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{33: JavaObject id=o430, 18: JavaObject id=o431, 26: JavaObject id=o432}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdds = spark._jsc.getPersistentRDDs()\n",
    "rdds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in our Data Science Jobs DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='Datasets/'\n",
    "videos = spark.read.csv(path+'youtubevideos.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this dataset\n",
    "\n",
    "This dataset includes several months of data on daily trending YouTube videos.\n",
    "\n",
    "**Source:** https://www.kaggle.com/datasnaek/youtube-new#USvideos.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2kyS6SvSYSE</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n",
       "      <td>CaseyNeistat</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-11-13T17:13:01.000Z</td>\n",
       "      <td>SHANtell martin</td>\n",
       "      <td>748374</td>\n",
       "      <td>57527</td>\n",
       "      <td>2966</td>\n",
       "      <td>15954</td>\n",
       "      <td>https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SHANTELL'S CHANNEL - https://www.youtube.com/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1ZAPwfrtAFY</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>The Trump Presidency: Last Week Tonight with J...</td>\n",
       "      <td>LastWeekTonight</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13T07:30:00.000Z</td>\n",
       "      <td>\"last week tonight trump presidency\"|\"last wee...</td>\n",
       "      <td>2418783</td>\n",
       "      <td>97185</td>\n",
       "      <td>6146</td>\n",
       "      <td>12703</td>\n",
       "      <td>https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>One year after the presidential election, John...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5qpjK5DgCt4</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Racist Superman | Rudy Mancuso, King Bach &amp; Le...</td>\n",
       "      <td>Rudy Mancuso</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-11-12T19:05:24.000Z</td>\n",
       "      <td>\"racist superman\"|\"rudy\"|\"mancuso\"|\"king\"|\"bac...</td>\n",
       "      <td>3191434</td>\n",
       "      <td>146033</td>\n",
       "      <td>5339</td>\n",
       "      <td>8181</td>\n",
       "      <td>https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>puqaWrEC7tY</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Nickelback Lyrics: Real or Fake?</td>\n",
       "      <td>Good Mythical Morning</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13T11:00:04.000Z</td>\n",
       "      <td>\"rhett and link\"|\"gmm\"|\"good mythical morning\"...</td>\n",
       "      <td>343168</td>\n",
       "      <td>10172</td>\n",
       "      <td>666</td>\n",
       "      <td>2146</td>\n",
       "      <td>https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Today we find out if Link is a Nickelback amat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id trending_date  \\\n",
       "0  2kyS6SvSYSE      17.14.11   \n",
       "1  1ZAPwfrtAFY      17.14.11   \n",
       "2  5qpjK5DgCt4      17.14.11   \n",
       "3  puqaWrEC7tY      17.14.11   \n",
       "\n",
       "                                               title          channel_title  \\\n",
       "0                 WE WANT TO TALK ABOUT OUR MARRIAGE           CaseyNeistat   \n",
       "1  The Trump Presidency: Last Week Tonight with J...        LastWeekTonight   \n",
       "2  Racist Superman | Rudy Mancuso, King Bach & Le...           Rudy Mancuso   \n",
       "3                   Nickelback Lyrics: Real or Fake?  Good Mythical Morning   \n",
       "\n",
       "  category_id              publish_time  \\\n",
       "0          22  2017-11-13T17:13:01.000Z   \n",
       "1          24  2017-11-13T07:30:00.000Z   \n",
       "2          23  2017-11-12T19:05:24.000Z   \n",
       "3          24  2017-11-13T11:00:04.000Z   \n",
       "\n",
       "                                                tags    views   likes  \\\n",
       "0                                    SHANtell martin   748374   57527   \n",
       "1  \"last week tonight trump presidency\"|\"last wee...  2418783   97185   \n",
       "2  \"racist superman\"|\"rudy\"|\"mancuso\"|\"king\"|\"bac...  3191434  146033   \n",
       "3  \"rhett and link\"|\"gmm\"|\"good mythical morning\"...   343168   10172   \n",
       "\n",
       "  dislikes comment_count                                  thumbnail_link  \\\n",
       "0     2966         15954  https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg   \n",
       "1     6146         12703  https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg   \n",
       "2     5339          8181  https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg   \n",
       "3      666          2146  https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg   \n",
       "\n",
       "  comments_disabled ratings_disabled video_error_or_removed  \\\n",
       "0             False            False                  False   \n",
       "1             False            False                  False   \n",
       "2             False            False                  False   \n",
       "3             False            False                  False   \n",
       "\n",
       "                                         description  \n",
       "0  SHANTELL'S CHANNEL - https://www.youtube.com/s...  \n",
       "1  One year after the presidential election, John...  \n",
       "2  WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...  \n",
       "3  Today we find out if Link is a Nickelback amat...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course the schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: string (nullable = true)\n",
      " |-- likes: string (nullable = true)\n",
      " |-- dislikes: string (nullable = true)\n",
      " |-- comment_count: string (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(videos.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right away we can see that a few of the variable types were not correctly infered. This is one of the common issues that you will run into in PySpark so in this lecture, we will learn how to correct it after the read in as opposed to during the read in. \n",
    "\n",
    "## Manipulate Data\n",
    "\n",
    "Being able to manipulate data is one of the most important skills for a data scientist to have whether you are doing machine learning or even simple reporting anlaytics. So will go over all the essentials here that you will need to do just that!\n",
    "\n",
    "*Note: Before you manipulate data, if you just want to test some code, you can use the .show() or .limit(6).toPandas() method as I've shown below. This will only display the results and NOT change your dataframe and also saves on computation time. *\n",
    "\n",
    "\n",
    "### Changing data types after read in\n",
    "First up, we see how to change data types. Many times, you will notice that Spark's \"handy\" infer schema is not quite so handy. So you'll end up needing to edit the data types after you read in a dataframe quite often. Here's how you do that.\n",
    "\n",
    "#### Available types:\n",
    "    - DataType\n",
    "    - NullType\n",
    "    - StringType\n",
    "    - BinaryType\n",
    "    - BooleanType\n",
    "    - DateType\n",
    "    - TimestampType\n",
    "    - DecimalType\n",
    "    - DoubleType\n",
    "    - FloatType\n",
    "    - ByteType\n",
    "    - IntegerType\n",
    "    - LongType\n",
    "    - ShortType\n",
    "    - ArrayType\n",
    "    - MapType\n",
    "    - StructField\n",
    "    - StructType\n",
    "    \n",
    "### Adding new columns to a dataframe or overwriting them\n",
    "\n",
    "You can use PySpark's .withColumn() method for both of these usecases. \n",
    "\n",
    "    Example (add new col): df.withColumn(\"double_age\",age*2)\n",
    "    Example (overwrite existing col): df.withColumn(\"age\",age*2)\n",
    "    \n",
    "Where the first value you pass in is the name of the new column and the second calls on the existing df column name you want to call on. You don't necessarily need to manipulate the variable here either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: date (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: integer (nullable = true)\n",
      " |-- likes: integer (nullable = true)\n",
      " |-- dislikes: integer (nullable = true)\n",
      " |-- comment_count: string (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2kyS6SvSYSE</td>\n",
       "      <td>2011-01-17</td>\n",
       "      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n",
       "      <td>CaseyNeistat</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-11-13T17:13:01.000Z</td>\n",
       "      <td>SHANtell martin</td>\n",
       "      <td>748374</td>\n",
       "      <td>57527</td>\n",
       "      <td>2966</td>\n",
       "      <td>15954</td>\n",
       "      <td>https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SHANTELL'S CHANNEL - https://www.youtube.com/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1ZAPwfrtAFY</td>\n",
       "      <td>2011-01-17</td>\n",
       "      <td>The Trump Presidency: Last Week Tonight with J...</td>\n",
       "      <td>LastWeekTonight</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13T07:30:00.000Z</td>\n",
       "      <td>\"last week tonight trump presidency\"|\"last wee...</td>\n",
       "      <td>2418783</td>\n",
       "      <td>97185</td>\n",
       "      <td>6146</td>\n",
       "      <td>12703</td>\n",
       "      <td>https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>One year after the presidential election, John...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5qpjK5DgCt4</td>\n",
       "      <td>2011-01-17</td>\n",
       "      <td>Racist Superman | Rudy Mancuso, King Bach &amp; Le...</td>\n",
       "      <td>Rudy Mancuso</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-11-12T19:05:24.000Z</td>\n",
       "      <td>\"racist superman\"|\"rudy\"|\"mancuso\"|\"king\"|\"bac...</td>\n",
       "      <td>3191434</td>\n",
       "      <td>146033</td>\n",
       "      <td>5339</td>\n",
       "      <td>8181</td>\n",
       "      <td>https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>puqaWrEC7tY</td>\n",
       "      <td>2011-01-17</td>\n",
       "      <td>Nickelback Lyrics: Real or Fake?</td>\n",
       "      <td>Good Mythical Morning</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13T11:00:04.000Z</td>\n",
       "      <td>\"rhett and link\"|\"gmm\"|\"good mythical morning\"...</td>\n",
       "      <td>343168</td>\n",
       "      <td>10172</td>\n",
       "      <td>666</td>\n",
       "      <td>2146</td>\n",
       "      <td>https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Today we find out if Link is a Nickelback amat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id trending_date  \\\n",
       "0  2kyS6SvSYSE    2011-01-17   \n",
       "1  1ZAPwfrtAFY    2011-01-17   \n",
       "2  5qpjK5DgCt4    2011-01-17   \n",
       "3  puqaWrEC7tY    2011-01-17   \n",
       "\n",
       "                                               title          channel_title  \\\n",
       "0                 WE WANT TO TALK ABOUT OUR MARRIAGE           CaseyNeistat   \n",
       "1  The Trump Presidency: Last Week Tonight with J...        LastWeekTonight   \n",
       "2  Racist Superman | Rudy Mancuso, King Bach & Le...           Rudy Mancuso   \n",
       "3                   Nickelback Lyrics: Real or Fake?  Good Mythical Morning   \n",
       "\n",
       "  category_id              publish_time  \\\n",
       "0          22  2017-11-13T17:13:01.000Z   \n",
       "1          24  2017-11-13T07:30:00.000Z   \n",
       "2          23  2017-11-12T19:05:24.000Z   \n",
       "3          24  2017-11-13T11:00:04.000Z   \n",
       "\n",
       "                                                tags    views   likes  \\\n",
       "0                                    SHANtell martin   748374   57527   \n",
       "1  \"last week tonight trump presidency\"|\"last wee...  2418783   97185   \n",
       "2  \"racist superman\"|\"rudy\"|\"mancuso\"|\"king\"|\"bac...  3191434  146033   \n",
       "3  \"rhett and link\"|\"gmm\"|\"good mythical morning\"...   343168   10172   \n",
       "\n",
       "   dislikes comment_count                                  thumbnail_link  \\\n",
       "0      2966         15954  https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg   \n",
       "1      6146         12703  https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg   \n",
       "2      5339          8181  https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg   \n",
       "3       666          2146  https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg   \n",
       "\n",
       "  comments_disabled ratings_disabled video_error_or_removed  \\\n",
       "0             False            False                  False   \n",
       "1             False            False                  False   \n",
       "2             False            False                  False   \n",
       "3             False            False                  False   \n",
       "\n",
       "                                         description  \n",
       "0  SHANTELL'S CHANNEL - https://www.youtube.com/s...  \n",
       "1  One year after the presidential election, John...  \n",
       "2  WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...  \n",
       "3  Today we find out if Link is a Nickelback amat...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice all vars are stings above....\n",
    "# let's change that\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import * # IntegerType\n",
    "\n",
    "df = videos.withColumn(\"views\", videos[\"views\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"likes\", videos[\"likes\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"dislikes\", videos[\"dislikes\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"trending_date\", to_date(videos.trending_date, 'dd.mm.yy')) \\\n",
    "#         .withColumn(\"publish_time\", to_timestamp(videos.publish_time, 'yyyy-MM-dd HH:mm:ss:ms'))\n",
    "print(df.printSchema())\n",
    "df.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Renaming Columns**\n",
    "\n",
    "If you simply needed to rename a column you use also use this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title_new</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "      <th>publish_time_2</th>\n",
       "      <th>publish_time_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2kyS6SvSYSE</td>\n",
       "      <td>2011-01-17</td>\n",
       "      <td>we want to talk about our marriage</td>\n",
       "      <td>CaseyNeistat</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-11-13T17:13:01.000Z</td>\n",
       "      <td>SHANtell martin</td>\n",
       "      <td>748374</td>\n",
       "      <td>57527</td>\n",
       "      <td>2966</td>\n",
       "      <td>15954</td>\n",
       "      <td>https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SHANTELL'S CHANNEL - https://www.youtube.com/s...</td>\n",
       "      <td>2017-11-13 17:13:01.000</td>\n",
       "      <td>2017-11-13 17:13:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1ZAPwfrtAFY</td>\n",
       "      <td>2011-01-17</td>\n",
       "      <td>the trump presidency: last week tonight with j...</td>\n",
       "      <td>LastWeekTonight</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13T07:30:00.000Z</td>\n",
       "      <td>\"last week tonight trump presidency\"|\"last wee...</td>\n",
       "      <td>2418783</td>\n",
       "      <td>97185</td>\n",
       "      <td>6146</td>\n",
       "      <td>12703</td>\n",
       "      <td>https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>One year after the presidential election, John...</td>\n",
       "      <td>2017-11-13 07:30:00.000</td>\n",
       "      <td>2017-11-13 07:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5qpjK5DgCt4</td>\n",
       "      <td>2011-01-17</td>\n",
       "      <td>racist superman | rudy mancuso, king bach &amp; le...</td>\n",
       "      <td>Rudy Mancuso</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-11-12T19:05:24.000Z</td>\n",
       "      <td>\"racist superman\"|\"rudy\"|\"mancuso\"|\"king\"|\"bac...</td>\n",
       "      <td>3191434</td>\n",
       "      <td>146033</td>\n",
       "      <td>5339</td>\n",
       "      <td>8181</td>\n",
       "      <td>https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...</td>\n",
       "      <td>2017-11-12 19:05:24.000</td>\n",
       "      <td>2017-11-12 19:05:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>puqaWrEC7tY</td>\n",
       "      <td>2011-01-17</td>\n",
       "      <td>nickelback lyrics: real or fake?</td>\n",
       "      <td>Good Mythical Morning</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13T11:00:04.000Z</td>\n",
       "      <td>\"rhett and link\"|\"gmm\"|\"good mythical morning\"...</td>\n",
       "      <td>343168</td>\n",
       "      <td>10172</td>\n",
       "      <td>666</td>\n",
       "      <td>2146</td>\n",
       "      <td>https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Today we find out if Link is a Nickelback amat...</td>\n",
       "      <td>2017-11-13 11:00:04.000</td>\n",
       "      <td>2017-11-13 11:00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id trending_date  \\\n",
       "0  2kyS6SvSYSE    2011-01-17   \n",
       "1  1ZAPwfrtAFY    2011-01-17   \n",
       "2  5qpjK5DgCt4    2011-01-17   \n",
       "3  puqaWrEC7tY    2011-01-17   \n",
       "\n",
       "                                               title      channel_title_new  \\\n",
       "0                 we want to talk about our marriage           CaseyNeistat   \n",
       "1  the trump presidency: last week tonight with j...        LastWeekTonight   \n",
       "2  racist superman | rudy mancuso, king bach & le...           Rudy Mancuso   \n",
       "3                   nickelback lyrics: real or fake?  Good Mythical Morning   \n",
       "\n",
       "  category_id              publish_time  \\\n",
       "0          22  2017-11-13T17:13:01.000Z   \n",
       "1          24  2017-11-13T07:30:00.000Z   \n",
       "2          23  2017-11-12T19:05:24.000Z   \n",
       "3          24  2017-11-13T11:00:04.000Z   \n",
       "\n",
       "                                                tags    views   likes  \\\n",
       "0                                    SHANtell martin   748374   57527   \n",
       "1  \"last week tonight trump presidency\"|\"last wee...  2418783   97185   \n",
       "2  \"racist superman\"|\"rudy\"|\"mancuso\"|\"king\"|\"bac...  3191434  146033   \n",
       "3  \"rhett and link\"|\"gmm\"|\"good mythical morning\"...   343168   10172   \n",
       "\n",
       "   dislikes comment_count                                  thumbnail_link  \\\n",
       "0      2966         15954  https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg   \n",
       "1      6146         12703  https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg   \n",
       "2      5339          8181  https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg   \n",
       "3       666          2146  https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg   \n",
       "\n",
       "  comments_disabled ratings_disabled video_error_or_removed  \\\n",
       "0             False            False                  False   \n",
       "1             False            False                  False   \n",
       "2             False            False                  False   \n",
       "3             False            False                  False   \n",
       "\n",
       "                                         description           publish_time_2  \\\n",
       "0  SHANTELL'S CHANNEL - https://www.youtube.com/s...  2017-11-13 17:13:01.000   \n",
       "1  One year after the presidential election, John...  2017-11-13 07:30:00.000   \n",
       "2  WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...  2017-11-12 19:05:24.000   \n",
       "3  Today we find out if Link is a Nickelback amat...  2017-11-13 11:00:04.000   \n",
       "\n",
       "       publish_time_3  \n",
       "0 2017-11-13 17:13:01  \n",
       "1 2017-11-13 07:30:00  \n",
       "2 2017-11-12 19:05:24  \n",
       "3 2017-11-13 11:00:04  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple Rename\n",
    "renamed = df.withColumnRenamed('channel_title','channel_title_new')\n",
    "renamed.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean Data**\n",
    "\n",
    "Alright so we see that the publish_time variable could not be converted to a timestamp becuase it has those strange \"T\" and \"Z\" values between the date and the time. We essentially need to replace the \"T\" value with a space, and the Z value with nothing. There are a couple of ways we can do this, the first is regex which is short for regular expressions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regex**\n",
    "\n",
    "Regex is used to replace or extract all substrings of the specified string value that match regexp with repetition.\n",
    "\n",
    "The syntax here is: regexp_replace(*str, pattern, replacement*)\n",
    "\n",
    "Regex is NOT super intuitive, so if you need a refresher on regex calls visit: \n",
    " - https://www.whoishostingthis.com/resources/regex/\n",
    " - https://docs.oracle.com/cd/B19306_01/server.102/b14200/ap_posix001.htm#BABJDBHB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: date (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: integer (nullable = true)\n",
      " |-- likes: integer (nullable = true)\n",
      " |-- dislikes: integer (nullable = true)\n",
      " |-- comment_count: string (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- publish_time_2: string (nullable = true)\n",
      " |-- publish_time_3: timestamp (nullable = true)\n",
      "\n",
      "None\n",
      "+------------------------+-----------------------+-------------------+\n",
      "|publish_time            |publish_time_2         |publish_time_3     |\n",
      "+------------------------+-----------------------+-------------------+\n",
      "|2017-11-13T17:13:01.000Z|2017-11-13 17:13:01.000|2017-11-13 17:13:01|\n",
      "|2017-11-13T07:30:00.000Z|2017-11-13 07:30:00.000|2017-11-13 07:30:00|\n",
      "|2017-11-12T19:05:24.000Z|2017-11-12 19:05:24.000|2017-11-12 19:05:24|\n",
      "|2017-11-13T11:00:04.000Z|2017-11-13 11:00:04.000|2017-11-13 11:00:04|\n",
      "|2017-11-12T18:01:41.000Z|2017-11-12 18:01:41.000|2017-11-12 18:01:41|\n",
      "+------------------------+-----------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, regexp_extract\n",
    "# import pyspark.sql.functions as f\n",
    "\n",
    "df = df.withColumn('publish_time_2',regexp_replace(df.publish_time, 'T', ' '))\n",
    "df = df.withColumn('publish_time_2',regexp_replace(df.publish_time_2, 'Z', ''))\n",
    "df = df.withColumn(\"publish_time_3\", to_timestamp(df.publish_time_2, 'yyyy-MM-dd HH:mm:ss.SSS'))\n",
    "print(df.printSchema())\n",
    "df.select(\"publish_time\", \"publish_time_2\",\"publish_time_3\").show(5,False)\n",
    "# Notice the .000 on the end of publish_time_new as opposed to publish_time_new_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translate Function**\n",
    "\n",
    "You could also use the Translate function here to do this, where the first set of values is what you are looking for and the second set is what you want to replace those values with respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------------------+\n",
      "|publish_time            |translate_func         |\n",
      "+------------------------+-----------------------+\n",
      "|2017-11-13T17:13:01.000Z|2017-11-13 17:13:01.000|\n",
      "|2017-11-13T07:30:00.000Z|2017-11-13 07:30:00.000|\n",
      "|2017-11-12T19:05:24.000Z|2017-11-12 19:05:24.000|\n",
      "|2017-11-13T11:00:04.000Z|2017-11-13 11:00:04.000|\n",
      "|2017-11-12T18:01:41.000Z|2017-11-12 18:01:41.000|\n",
      "+------------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "df.select(\"publish_time\",f.translate(f.col(\"publish_time\"), \"TZ\", \" \").alias(\"translate_func\")).show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trim**\n",
    "\n",
    "One common function you've probably seen in almost any data processing tool including excel is the \"trim\" function which removes leading and trailing white space from a cell in various ways. Let's go ahead and do that with the title field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+\n",
      "|title                                                         |\n",
      "+--------------------------------------------------------------+\n",
      "|WE WANT TO TALK ABOUT OUR MARRIAGE                            |\n",
      "|The Trump Presidency: Last Week Tonight with John Oliver (HBO)|\n",
      "|Racist Superman | Rudy Mancuso, King Bach & Lele Pons         |\n",
      "|Nickelback Lyrics: Real or Fake?                              |\n",
      "|I Dare You: GOING BALD!?                                      |\n",
      "+--------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trim\n",
    "# pyspark.sql.functions.trim(col) - Trim the spaces from both ends for the specified string column.\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = df.withColumn('title',trim(df.title)) # or rtrim/ltrim\n",
    "df.select(\"title\").show(5,False)\n",
    "\n",
    "# trim_ex = spark.createDataFrame([(' 2015-04-08 ',' 2015-05-10 ')], ['d1', 'd2']) # create a dataframe - notice the extra whitespaces in the date strings\n",
    "# trim_ex.show()\n",
    "# print(\"left trim\")\n",
    "# trim_ex.select('d1', ltrim(trim_ex.d1)).show()\n",
    "# print(\"right trim\")\n",
    "# trim_ex.select('d1', rtrim(trim_ex.d1)).show()\n",
    "# print(\"trim\")\n",
    "# trim_ex.select('d1', trim(trim_ex.d1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lower**\n",
    "\n",
    "Another common data cleaning technique is lower casing all values in a string. Here's how we could do that.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+\n",
      "|title                                                         |\n",
      "+--------------------------------------------------------------+\n",
      "|we want to talk about our marriage                            |\n",
      "|the trump presidency: last week tonight with john oliver (hbo)|\n",
      "|racist superman | rudy mancuso, king bach & lele pons         |\n",
      "|nickelback lyrics: real or fake?                              |\n",
      "|i dare you: going bald!?                                      |\n",
      "+--------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('title',lower(df.title)) # or rtrim/ltrim\n",
    "df.select(\"title\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case when**\n",
    "\n",
    "We can also use the classic sql \"case when\" clause to recode values. Let's say we wanted to create a categorical variable that told if the video had more likes than dislikes and visa versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option#1: select or withColumn() using when-otherwise\n",
      "+------+--------+------------+\n",
      "| likes|dislikes|Favorability|\n",
      "+------+--------+------------+\n",
      "| 57527|    2966|        Good|\n",
      "| 97185|    6146|        Good|\n",
      "|146033|    5339|        Good|\n",
      "+------+--------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Option2: select or withColumn() using expr function\n",
      "+------+--------+------------+\n",
      "| likes|dislikes|Favorability|\n",
      "+------+--------+------------+\n",
      "| 57527|    2966|        Good|\n",
      "| 97185|    6146|        Good|\n",
      "|146033|    5339|        Good|\n",
      "+------+--------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Option 3: selectExpr() using SQL equivalent CASE expression\n",
      "+------+--------+------------+\n",
      "| likes|dislikes|Favorability|\n",
      "+------+--------+------------+\n",
      "| 57527|    2966|        Good|\n",
      "| 97185|    6146|        Good|\n",
      "|146033|    5339|        Good|\n",
      "+------+--------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Option#1: select or withColumn() using when-otherwise\")\n",
    "from pyspark.sql.functions import when\n",
    "df.select(\"likes\",\"dislikes\",(when(df.likes > df.dislikes, 'Good').when(df.likes < df.dislikes, 'Bad').otherwise('Undetermined')).alias(\"Favorability\")).show(3)\n",
    "\n",
    "print(\"Option2: select or withColumn() using expr function\")\n",
    "from pyspark.sql.functions import expr \n",
    "df.select(\"likes\",\"dislikes\",expr(\"CASE WHEN likes > dislikes THEN  'Good' WHEN likes < dislikes THEN 'Bad' ELSE 'Undetermined' END AS Favorability\")).show(3)\n",
    "\n",
    "print(\"Option 3: selectExpr() using SQL equivalent CASE expression\")\n",
    "df.selectExpr(\"likes\",\"dislikes\",\"CASE WHEN likes > dislikes THEN  'Good' WHEN likes < dislikes THEN 'Bad' ELSE 'Undetermined' END AS Favorability\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatenate**\n",
    "\n",
    "If you want to combine two variables together (given a separator) you can use the concatenate method. Let's say we wanted to combined all the text description variables of the videos here for a robust NLP exercise of some sort and we needed to have all the text in one colum to do that like this.\n",
    "\n",
    "    concat_ws(sep, *cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+\n",
      "|text                                                           |\n",
      "+---------------------------------------------------------------+\n",
      "|we want to talk about our marriage CaseyNeistat SHANtell martin|\n",
      "+---------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "df.select(concat_ws(' ', df.title,df.channel_title,df.tags).alias('text')).show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting data from Date and Timestamp variables**\n",
    "\n",
    "If you have the need to extract say the year or month from a date field, you can use PySpark's SQL function library like this. \n",
    "\n",
    "Note with this analysis we stumbled apon a date conversion descrepancy here. I'll leave fixing that for a hw problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------------------+\n",
      "|trending_date|year(trending_date)|month(trending_date)|\n",
      "+-------------+-------------------+--------------------+\n",
      "|   2011-01-17|               2011|                   1|\n",
      "|   2011-01-17|               2011|                   1|\n",
      "|   2011-01-17|               2011|                   1|\n",
      "|   2011-01-17|               2011|                   1|\n",
      "|   2011-01-17|               2011|                   1|\n",
      "+-------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month\n",
    "# Other options: dayofmonth, dayofweek, dayofyear, weekofyear\n",
    "df.select(\"trending_date\",year(\"trending_date\"),month(\"trending_date\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the Difference between two dates**\n",
    "\n",
    "If you want to calculate the time difference between two dates, you could use PySparks datediff function which returns the number of days from start to end.\n",
    "\n",
    "    datediff(end, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-------------------+\n",
      "|trending_date|     publish_time_3|               diff|\n",
      "+-------------+-------------------+-------------------+\n",
      "|   2011-01-17|2017-11-13 17:13:01|-6.8273972602739725|\n",
      "|   2011-01-17|2017-11-13 07:30:00|-6.8273972602739725|\n",
      "|   2011-01-17|2017-11-12 19:05:24| -6.824657534246575|\n",
      "|   2011-01-17|2017-11-13 11:00:04|-6.8273972602739725|\n",
      "|   2011-01-17|2017-11-12 18:01:41| -6.824657534246575|\n",
      "+-------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "df.select(\"trending_date\",\"publish_time_3\",(datediff(df.trending_date,df.publish_time_3)/365).alias('diff')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split a string around a pattern**\n",
    "\n",
    "If you ever need to split a string on a pattern (where the pattern is a regex), you could use PySparks split function. You could actually use this for tokenizing text which is an NLP function that we'll get into later.\n",
    "\n",
    "    df.select(split(str, pattern))\n",
    "    \n",
    "*Note that this will create an array*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|title                             |\n",
      "+----------------------------------+\n",
      "|we want to talk about our marriage|\n",
      "+----------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------------------------------------------+\n",
      "|new                                       |\n",
      "+------------------------------------------+\n",
      "|[we, want, to, talk, about, our, marriage]|\n",
      "+------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split a string around pattern (pattern is a regular expression).\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "df.select(\"title\").show(1,False)\n",
    "df.select(split(df.title, ' ').alias('new')).show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Working with Arrays**\n",
    "\n",
    "    df.select(array_contains(df.variable, \"marriage\"))\n",
    "\n",
    "*note this is only available in pyspark 2.4+* \n",
    "    \n",
    "\n",
    " - .array(*cols)   -   Creates a new array column.\n",
    " - .array_contains(col, value)  - Collection function: returns null if the array is null, true if the array contains the given value, and false otherwise.\n",
    " - .array_distinct(col) - Collection function: removes duplicate values from the array. :param col: name of column or expression\n",
    " - .array_except(col1, col2) - Collection function: returns an array of the elements in col1 but not in col2, without duplicates.\n",
    " - .array_intersect(col1, col2) - Collection function: returns an array of the elements in the intersection of col1 and col2, without duplicates.\n",
    " - .array_join(col, delimiter, null_replacement=None) - Concatenates the elements of column using the delimiter. Null values are replaced with null_replacement if set, otherwise they are ignored.\n",
    " - .array_max(col) - Collection function: returns the maximum value of the array.\n",
    " - .array_min(col) - Collection function: returns the minimum value of the array.\n",
    " - .array_position(col, value) - Collection function: Locates the position of the first occurrence of the given value in the given array. Returns null if either of the arguments are null.\n",
    " - .array_remove(col, element)- Collection function: Remove all elements that equal to element from the given array.\n",
    " - .array_repeat(col, count) - Collection function: creates an array containing a column repeated count times.\n",
    " - .array_sort(col) - Collection function: sorts the input array in ascending order. The elements of the input array must be orderable. Null elements will be placed at the end of the returned array.\n",
    " - .array_union(col1, col2) - Collection function: returns an array of the elements in the union of col1 and col2, without duplicates.\n",
    " - .arrays_overlap(a1, a2) - Collection function: returns true if the arrays contain any common non-null element; if not, returns null if both the arrays are non-empty and any of them contains a null element; returns false otherwise.\n",
    " - .arrays_zip(*cols)[source] - Collection function: Returns a merged array of structs in which the N-th struct contains all N-th values of input arrays.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-------------------------------------+\n",
      "|title                             |array_contains(title_array, marriage)|\n",
      "+----------------------------------+-------------------------------------+\n",
      "|we want to talk about our marriage|true                                 |\n",
      "+----------------------------------+-------------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------------------------------------------+\n",
      "|array_distinct(title_array)               |\n",
      "+------------------------------------------+\n",
      "|[we, want, to, talk, about, our, marriage]|\n",
      "+------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------------------------+\n",
      "|array_remove(title_array, we)         |\n",
      "+--------------------------------------+\n",
      "|[want, to, talk, about, our, marriage]|\n",
      "+--------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "array_df = df.select(\"title\",split(df.title, ' ').alias('title_array'))\n",
    "\n",
    "array_df.select(\"title\",array_contains(array_df.title_array, \"marriage\")).show(1, False)\n",
    "\n",
    "#get rid of repeat values\n",
    "array_df.select(array_distinct(array_df.title_array)).show(1, False)\n",
    "\n",
    "# Remove certian values\n",
    "array_df.select(array_remove(array_df.title_array, \"we\")).show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Functions\n",
    "\n",
    "Functions as you know them in Python work a bit differently in Pyspark because it operates on a cluster. If you define a function the traditional Python way in PySpark, you will not recieve an error message but the call will not distribute on all nodes. So it will run slower. \n",
    "\n",
    "So to convert a Python function to what's called a user defined function (UDF) in PySpark. This is what you do.\n",
    "\n",
    "*Note: keep in mind that a function will not work on a column with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|dislikes|likes_sq|\n",
      "+--------+--------+\n",
      "|    2966| 8797156|\n",
      "|    6146|37773316|\n",
      "|    5339|28504921|\n",
      "|     666|  443556|\n",
      "|    1989| 3956121|\n",
      "|     511|  261121|\n",
      "|    2445| 5978025|\n",
      "|     778|  605284|\n",
      "|     119|   14161|\n",
      "|    1363| 1857769|\n",
      "|      25|     625|\n",
      "|     303|   91809|\n",
      "|    1333| 1776889|\n",
      "|    1171| 1371241|\n",
      "|     246|   60516|\n",
      "|      52|    2704|\n",
      "|     638|  407044|\n",
      "|      53|    2809|\n",
      "|      36|    1296|\n",
      "|     191|   36481|\n",
      "+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def square(x):\n",
    "    return int(x**2)\n",
    "square_udf = udf(lambda z: square(z), IntegerType())\n",
    "\n",
    "df.select('dislikes',square_udf('dislikes').alias('likes_sq')).where(col('dislikes').isNotNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's all folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
