{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Writing and Validating Data in PySpark\n",
    "\n",
    "Welcome to PySpark!\n",
    "\n",
    "In this first lecture, we will be covering:\n",
    "\n",
    " - Reading in Data\n",
    " - Partioned Files\n",
    " - Validating Data\n",
    " - Specifying Data Types\n",
    " - Writing Data\n",
    "\n",
    "Below you will see the script to begin your first PySpark instance. If you're ever curious about how your PySpark instance is performing, Spark offers a neat Web UI with tons of information. Just navigate to http://[driver]:4040 in your browswer where \"drive\" is you driver name. If you are running PySpark locally, it would be http://localhost:4040 or you can use the hyperlink automatically produced from the script below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working with 1 core(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://orcuns-mbp-2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ReadWriteVal</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe3ca95f890>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's create our PySpark instance!\n",
    "\n",
    "# PC users can use the next two lines of code but mac users don't need it\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"ReadWriteVal\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "\n",
    "A DataFrame is equivalent to a relational table in Spark SQL, and can be created using various functions in SparkSession.\n",
    "\n",
    "First let's try reading in a csv file containing a list of students and their grades.\n",
    "\n",
    "**Source:** https://www.kaggle.com/spscientist/students-performance-in-exams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by reading a basic csv dataset\n",
    "# Let Spark know about the header and infer the Schema types!\n",
    "\n",
    "path =\"Datasets/\"\n",
    "\n",
    "# Some csv data\n",
    "students = spark.read.csv(path+'students.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parquet Files**\n",
    "\n",
    "Now try reading in a parquet file. This is most common data type in the big data world.\n",
    "Why? because it is the most compact file storage method (even better than zipped files!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet = spark.read.parquet(path+'users1.parquet')\n",
    "parquet.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Partitioned Parquet Files**\n",
    "\n",
    "Actually most big datasets will be partitioned. Here is how you can collect all the pieces (parts) of the dataset in one simple command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned = spark.read.parquet(path+'users*')\n",
    "partitioned.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also opt to read in only a specific set of paritioned parquet files. Say for example that you only wanted users1 and users2 and not users3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the .option(\"basePath\", path) option is used to override the automatic function\n",
    "# that will exclude the partitioned variable in resulting dataframe. \n",
    "# I prefer to have the partitioning info in my new dataframe personally. \n",
    "users1_2 = spark.read.option(\"basePath\", path).parquet(path+'users1.parquet', path+'users2.parquet')\n",
    "users1_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you're in AWS storing data in s3 buckets your code will more like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"my_bucket\"\n",
    "key1 = \"partition_test/Table1/CREATED_YEAR=2015/*\"\n",
    "key2 = \"partition_test/Table1/CREATED_YEAR=2017/*\"\n",
    "key3 = \"partition_test/Table1/CREATED_YEAR=2018/*\"\n",
    "\n",
    "test_df = spark.read.parquet('s3://'+bucket+'/'+key1,\\\n",
    "                             's3://'+bucket+'/'+key2, \\\n",
    "                             's3://'+bucket+'/'+key3)\n",
    "\n",
    "test_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Data\n",
    "\n",
    "Next you will want to validate that you dataframe was read in correct. We will get into more detailed data evaluation later on but first we need to ensure that all the variable types were infered correctly and that the values actually made it in... sometimes they don't :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an inital view of your dataframe\n",
    "students.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your dataframe is more than just a few variables, this method is way better\n",
    "students.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Note the types here:\n",
    "print(type(students))\n",
    "studentsPdf = students.toPandas()\n",
    "print(type(studentsPdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Solid Summary of your data:\n",
    "\n",
    "#show the data (like df.head())\n",
    "print(students.printSchema())\n",
    "print(\"\")\n",
    "print(students.columns)\n",
    "print(\"\")\n",
    "print(students.describe()) # Not so fond of this one but to each their own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to get the type of just ONE column by name you can use this function:\n",
    "students.schema['math score'].dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neat \"describe\" function\n",
    "students.describe(['math score']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary function\n",
    "students.select(\"math score\", \"reading score\",\"writing score\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to specify data types as you read in datasets.\n",
    "\n",
    "Some data types make it easier to infer schema (like tabular formats such as csv which we will show later). \n",
    "\n",
    "However you often have to set the schema yourself if you aren't dealing with a .read method that doesn't have inferSchema() built-in.\n",
    "\n",
    "Spark has all the tools you need for this, it just requires a very specific structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField,StringType,IntegerType,StructType,DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create the list of Structure fields\n",
    "    * :param name: string, name of the field.\n",
    "    * :param dataType: :class:`DataType` of the field.\n",
    "    * :param nullable: boolean, whether the field can be null (None) or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructField(\"name\", StringType(), True),\n",
    "               StructField(\"email\", StringType(), True),\n",
    "               StructField(\"city\", StringType(), True),\n",
    "               StructField(\"mac\", StringType(), True),\n",
    "               StructField(\"timestamp\", DateType(), True),\n",
    "               StructField(\"creditcard\", StringType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_struc = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do a .json file this time :) \n",
    "\n",
    "**Source:** https://gist.github.com/raine/da15845f332a2fb8937b344504abfbe0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = spark.read.json(path+'people.json', schema=final_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Data\n",
    "\n",
    "First let's just try writing a simple csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the funky naming convention of the file in your output folder. There is no way to directly change this. \n",
    "students.write.mode(\"overwrite\").csv('write_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# students.write.csv('write_test.csv')\n",
    "students.toPandas().to_csv('write_test2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the strange naming convention of the output file in the path that you specified. Spark uses Hadoop File Format, which requires data to be partitioned - that's why you have part- files. If you want to rename your written files to a more user friendly format, you can do that using the method below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py4j.java_gateway import java_import\n",
    "java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
    "\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "file = fs.globStatus(spark._jvm.Path('write_test.csv/part*'))[0].getPath().getName()\n",
    "fs.rename(spark._jvm.Path('write_test.csv/' + file), spark._jvm.Path('write_test2.csv')) #these two need to be different\n",
    "fs.delete(spark._jvm.Path('write_test.csv'), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writting Parquet files\n",
    "\n",
    "Now let's try writing a parquet file. This is best practice for big data as it is the most compact storage method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users1_2.write.mode(\"overwrite\").parquet('parquet/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those who got an error attempting to run the above code. Try this solution: https://stackoverflow.com/questions/59220832/unable-to-write-spark-dataframe-to-a-parquet-file-format-to-c-drive-in-pyspark\n",
    "\n",
    "#### Writting Partitioned Parquet Files\n",
    "\n",
    "Now try to write a partioned parquet file... super fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users1_2.write.mode(\"overwrite\").partitionBy(\"gender\").parquet('part_parquet/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writting your own dataframes here!\n",
    "\n",
    "You can also create your own dataframes directly here in your Juypter Notebook too if you want. \n",
    "\n",
    "Like this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [('Pear',10),('Orange',36),('Banana',123),('Kiwi',48),('Peach',16),('Strawberry',1)]\n",
    "df = spark.createDataFrame(values,['fruit','quantity'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That's it! Great job"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
